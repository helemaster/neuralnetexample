{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network Example: Handwriting Recognition</h1><br/>\n",
    "Prepared by <a href=\"mailto:HELeMaster@my.harrisburgu.edu\">Holly LeMaster</a><br/> \n",
    "Harrisburg University of Science and Technology<br />\n",
    "Based on <i>Make your Own Neural Network</i> by T. Rashid\n",
    "\n",
    "This notebook will demonstrate a simple neural network and utilize it to analyze writing samples. Based on a selection of hand-written numbers, the neural network will determine which digit they represent. <br /><br />\n",
    "\n",
    "<b>Ensuring this notebook works correctly:</b><br />\n",
    "Please make sure the directory of this notebook contains the following files:\n",
    "<ul>\n",
    "    <li><code>data</code> directory, containing dataset files</li>\n",
    "    <li><code>media</code> directory, containing all images</li>\n",
    "</ul>\n",
    "\n",
    "You can download the training and test MNIST sets from the following links:\n",
    "<ul>\n",
    "    <li><a href=\"https://www.google.com/url?q=http://www.pjreddie.com/media/files/mnist_train.csv&sa=D&ust=1459380918475000&usg=AFQjCNHGH44RvgkyjB1suF264J4YLaXWJA\">Training</a></li>\n",
    "    <li><a href=\"https://www.google.com/url?q=http://www.pjreddie.com/media/files/mnist_test.csv&sa=D&ust=1459380918476000&usg=AFQjCNFmB73OHhZ9WaPrLzEdVw_bfBJG3Q\">Testing</a></li>\n",
    "</ul>\n",
    "\n",
    "<br />\n",
    "<i>Â© Tariq Rashid, 2016</i><br />\n",
    "<i>GPLv2 license</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.special as special\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the necessary libraries. \n",
    "\n",
    "<code><font color=\"orange\">numpy</font></code> is a scientific computing library with many useful functions for data manipulation. See the documentation <a href=\"https://docs.scipy.org/doc/numpy/\">here</a>.\n",
    "\n",
    "<code><font color=\"orange\">scipy</font></code> is a scientific computing library. The documentation can be found <a href=\"https://docs.scipy.org/doc/scipy/reference/\">here</a>. We use this library specifically for its special module, which includes a variety of useful pre-defined functions. In particular, we want the sigmoid function.\n",
    "\n",
    "<code><font color=\"orange\">matplotlib</font></code> provides graphing and plotting functionality. The documentation is <a href=\"https://matplotlib.org/contents.html\">here</a>. We include <code>%matplotlib inline</code> to ensure the graphs are printed under code cells rather than opening in a separate window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NeuralNet class - representation of a basic neural network\n",
    "class NeuralNet:\n",
    "    #Initialize network\n",
    "    def __init__(self, inputNodes, hiddenNodes, outputNodes, learningRate):\n",
    "        self.inodes = inputNodes\n",
    "        self.hnodes = hiddenNodes\n",
    "        self.onodes = outputNodes\n",
    "        self.lr = learningRate\n",
    "        \n",
    "        #Init random link weight matricies\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        #Define anonymous activiation function using lambda shorthand\n",
    "        self.activation = lambda x: special.expit(x)\n",
    "        \n",
    "    #train - trains the neural netwwork\n",
    "    def train(self, inputsList, targetsList):\n",
    "        #Convert inputs to 2d numpy array\n",
    "        inputs = numpy.array(inputsList, ndmin=2).T\n",
    "        targets = numpy.array(targetsList, ndmin=2).T\n",
    "        \n",
    "        #Calculate signals into hidden layer\n",
    "        hiddenInputs = numpy.dot(self.wih, inputs)\n",
    "        \n",
    "        #Apply activation function to signals from hidden layer\n",
    "        hiddenOutputs = self.activation(hiddenInputs)\n",
    "        \n",
    "        #Calculate signals into final output layer\n",
    "        finalInputs = numpy.dot(self.who, hiddenOutputs)\n",
    "        \n",
    "        #Apply activation function to final outputs \n",
    "        finalOutputs = self.activation(finalInputs)\n",
    "        \n",
    "        #Calculate error: (target - actual)\n",
    "        #output errors\n",
    "        outputErrors = targets - finalOutputs\n",
    "        \n",
    "        #Backpropagate errors from output layer\n",
    "        hiddenErrors = numpy.dot(self.who.T, outputErrors)\n",
    "        \n",
    "        #Update weights for links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((outputErrors * finalOutputs * (1.0 - finalOutputs)), numpy.transpose(hiddenOutputs))\n",
    "        \n",
    "        #Update weights for links between input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hiddenErrors * hiddenOutputs * (1.0 - hiddenOutputs)), numpy.transpose(inputs))\n",
    "        \n",
    "    \n",
    "    #query - queries the neural network\n",
    "    #Takes input to neural network and returns network's output\n",
    "    def query(self, inputsList):\n",
    "        \n",
    "        #Convert input list to 2d numpy array\n",
    "        inputs = numpy.array(inputsList, ndmin=2).T\n",
    "        \n",
    "        #Use dot product matrix multiplication to calculate outputs\n",
    "        #Combines all inputs with link weights to produce a matrix of combined moderated signals into each hidden layer node\n",
    "        hiddenInputs = numpy.dot(self.wih, inputs)\n",
    "\n",
    "        #Apply activation function to signals from hidden layer\n",
    "        hiddenOutputs = self.activation(hiddenInputs)\n",
    "        \n",
    "        #Calculate signals into final output layer\n",
    "        finalInputs = numpy.dot(self.who, hiddenOutputs)\n",
    "        \n",
    "        #Calculate signals from final output layer (apply act. func)\n",
    "        finalOutputs = self.activation(finalInputs)\n",
    "                \n",
    "        return finalOutputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have implemented the neural network as a class. In its simplest form, a neural network accepts inputs, performs some function(s) on them, and returns an output. See <b>Figure 1</b> below, which depicts a single node from a neural network. Following the outputs, we calculate error and update link weights in the neural network to reflect changes we want to see. \n",
    "\n",
    "Links are the lines that connect the various nodes and each has a weight associated with it. We can adjust these weights to drive the learning of the network.\n",
    "\n",
    "For a deeper dive into the inner-workings of a neural network, see the text this notebook is based on, <i>Build Your Own Neural Network</i>.\n",
    "\n",
    "<img src=\"media/node.png\">\n",
    "<div align=\"center\"><b>Figure 1:</b> Neural Network Node</div>\n",
    "\n",
    "Note that we apply a sigmoid function to our calculations. The purpose of this is to make the calculations \"fuzzy\" - neural networks are designed to mimic neurons in organic creatures' brains, and the sigmoid function modulates results to imitate this.\n",
    "\n",
    "Due to limitations with Jupyter, the entire class must be in one cell. A breakdown of each section of the class can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Class Breakdown</h2>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class NeuralNet:\n",
    "    #Initialize network\n",
    "    def __init__(self, inputNodes, hiddenNodes, outputNodes, learningRate):\n",
    "        self.inodes = inputNodes\n",
    "        self.hnodes = hiddenNodes\n",
    "        self.onodes = outputNodes\n",
    "        self.lr = learningRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class is initialized with input nodes, hidden nodes, output nodes, and a learning rate. The input nodes accept data and pass it to the inner \"hidden\" nodes, which pass their data to the output nodes. Each node applies functions to the data.\n",
    "\n",
    "The learning rate is a parameter that determines how much link weights are adjusted. Lower values cause slower learning, while higher values accelerated it. It's necessary to find a good balance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        #Init random link weight matricies\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        #Define anonymous activiation function using lambda shorthand\n",
    "        self.activation = lambda x: special.expit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last section of the constructor function initializes the link weights. Assignment is initially arbitrary and is adjusted later. We store these in <code>self.wih</code> (weight of links between input & hidden nodes) and <code>self.who</code> (weight of links between hidden & output nodes).\n",
    "\n",
    "To close the constructor off, we define the activiation function we pulled from <code><font color='orange'>scipy</font></code> using Python's <code><font color='green'>lambda</font></code> function shorthand. You can read more about lambda <a href=\"https://www.w3schools.com/python/python_lambda.asp\">here.</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    #train - trains the neural netwwork\n",
    "    def train(self, inputsList, targetsList):\n",
    "        #Convert inputs to 2d numpy array\n",
    "        inputs = numpy.array(inputsList, ndmin=2).T\n",
    "        targets = numpy.array(targetsList, ndmin=2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function we define for the class is the training method. This function is what allows the neural network to learn. It accepts data, pushes it through the nodes, calculates error, and updates link weights. This is the core of our neural network.\n",
    "\n",
    "The function takes a list of input values (training data) and a list of target values (our goals / class labels). Note that these are matricies of data, and we will utilize matrix arithmetic to perform calculations. Refer to the text to see in detail how these calculations work.\n",
    "\n",
    "We begin by converting the arguments to 2D <code><font color='orange'>numpy</font></code> arrays using the <code><font color='green'>array</font></code> function. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        #Calculate signals into hidden layer\n",
    "        hiddenInputs = numpy.dot(self.wih, inputs)\n",
    "        \n",
    "        #Apply activation function to signals from hidden layer\n",
    "        hiddenOutputs = self.activation(hiddenInputs)\n",
    "        \n",
    "        #Calculate signals into final output layer\n",
    "        finalInputs = numpy.dot(self.who, hiddenOutputs)\n",
    "        \n",
    "        #Apply activation function to final outputs \n",
    "        finalOutputs = self.activation(finalInputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the node calculations. We send the data through the input nodes and calculate the results sent to the hidden nodes. Since these are matricies, we use <code><font color='green'>dot</font></code> to calculate to dot product. \n",
    "\n",
    "Before the data can be considered output, we must apply the sigmoid activation function. We use the <code>self.activation</code> function we defined earlier and store those results in our hidden node outputs, <code>hiddenOutputs</code>.\n",
    "\n",
    "We repeat the process with the output nodes, this time sending them the outputs from the hidden nodes. Dot product multiplication is used to calculate the values of the output nodes, and the activation function is applied to get he final outputs.\n",
    "\n",
    "We now have a set of final outputs for our neural network! The next step is to determine the error and how the network can improve itself."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        #Calculate error: (target - actual)\n",
    "        #output errors\n",
    "        outputErrors = targets - finalOutputs\n",
    "        \n",
    "        #Backpropagate errors from output layer\n",
    "        hiddenErrors = numpy.dot(self.who.T, outputErrors)\n",
    "        \n",
    "        #Update weights for links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((outputErrors * finalOutputs * (1.0 - finalOutputs)), numpy.transpose(hiddenOutputs))\n",
    "        \n",
    "        #Update weights for links between input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hiddenErrors * hiddenOutputs * (1.0 - hiddenOutputs)), numpy.transpose(inputs))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the error is key in improving our neural network. To do so, we compare the targets to the final outputs following the formula <code>error = (targets - actual)</code>. We store this in <code>outputErrors</code> to be used in backpropagation.\n",
    "\n",
    "Backpropagation is used to determine the error of nodes that are in the middle layers. Since we only compare our target values with the final outputs, we get no feedback on the performance of the middle layer, the hidden nodes. Backpropagation allows us to calculate the error for those layers using the error of the final layer.\n",
    "\n",
    "We take the dot product of the matricies containing the hidden-output layer links and the output layer errors. This gives us error for the hidden layer. \n",
    "\n",
    "For more detailed information on backpropagation, refer to the text.\n",
    "\n",
    "Now that we have the error measures, we need to use these to update the link weights of the neural network. We first modify the hidden-output layer link weights, and then move on to the hidden-input layer links. The equation for this is as follows:\n",
    "\n",
    "<img src=\"media/weightUpdate.png\">\n",
    "<div align=\"center\"><b>Figure 2:</b> Link Weight Update Equation</div>\n",
    "\n",
    "where <i>j</i> is a node in the current layer, <i>k</i> is a node in the next layer, $\\alpha$ is the learning rate, <i>E</i> is error, <i>W</i> refers to the weights, and <i>O</i> represents output signals. This equation is translated into our code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    def query(self, inputsList):\n",
    "\n",
    "            #Convert input list to 2d numpy array\n",
    "            inputs = numpy.array(inputsList, ndmin=2).T\n",
    "            \n",
    "            #Use dot product matrix multiplication to calculate outputs\n",
    "            #Combines all inputs with link weights to produce a matrix of combined moderated signals into each hidden layer node\n",
    "            hiddenInputs = numpy.dot(self.wih, inputs)\n",
    "\n",
    "            #Apply activation function to signals from hidden layer\n",
    "            hiddenOutputs = self.activation(hiddenInputs)\n",
    "\n",
    "            #Calculate signals into final output layer\n",
    "            finalInputs = numpy.dot(self.who, hiddenOutputs)\n",
    "\n",
    "            #Calculate signals from final output layer (apply act. func)\n",
    "            finalOutputs = self.activation(finalInputs)\n",
    "\n",
    "            return finalOutputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function of the neural network is responsible for accepting input and returning output. Essentially, this is where all the processing is done. The function takes inputs in the form of a list.\n",
    "\n",
    "First, the input list is transformed into a 2D <code><font color='orange'>numpy</font></code> array. Next, the outputs of the hidden layer are calculated using matrix multiplication. The activation function is applied to those outputs. We then send the outputs from the hidden layer to the final layer using the same calculations. These final outputs are then returned. \n",
    "\n",
    "That concludes the neural network class! We can adapt this simple class for a variety of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initialization and Training</h2> \n",
    "\n",
    "Now that the neural network is implemented, we can test its power on a handwriting recognition application. In this case, we'll use the MNIST database, a dataset that contains handwritten numbers. \n",
    "\n",
    "MNIST is a freely-available dataset that contains 60,000 labeled examples of handwritten numbers. By labeled, we mean that the desired output (the target) comes along with the records. We'll be using the dataset in the CSV format, a plain-text file of comma-separated values.\n",
    "\n",
    "Let's take a look at one line from the dataset:\n",
    "\n",
    "<code>5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,18,18,18,126,136,175,26,166,255,247,127,0,0,0,0,0,0,0,0,0,0,0,0,30,36,94,154,170,253,253,253,253,253,225,172,253,242,195,64,0,0,0,0,0,0,0,0,0,0,0,49,238,253,253,253,253,253,253,253,253,251,93,82,82,56,39,0,0,0,0,0,0,0,0,0,0,0,0,18,219,253,253,253,253,253,198,182,247,241,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,80,156,107,253,253,205,11,0,43,154,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,1,154,253,90,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,139,253,190,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,190,253,70,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,35,241,225,160,108,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,240,253,253,119,25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,186,253,253,150,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,93,252,253,187,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,249,253,249,64,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,46,130,183,253,253,207,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,39,148,229,253,253,253,250,182,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,24,114,221,253,253,253,253,201,78,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,66,213,253,253,253,253,198,81,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,18,171,219,253,253,253,253,195,80,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,55,172,226,253,253,253,253,244,133,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,136,253,253,253,212,135,132,16,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "</code>\n",
    "\n",
    "While it looks overwhelming, it's easy to break down. The first number of the record, <code>5</code>, is our target. This means that the array of numbers we see here represents a five. The remaining numbers are pixels values for a handwritten digit image. The image is a 28x28 pixel array (784 individual values). The higher the number, the darker that pixel is.\n",
    "\n",
    "We can plot this data with <code><font color='orange'>matplotlib</font></code> to get a better idea of what the data represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "trainingDataFile = open(\"data/mnist_train.csv\", \"r\")\n",
    "trainingData = trainingDataFile.readlines()\n",
    "trainingDataFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before plotting, we import the necessary data. The MNIST data is imported from the CSV using Python file opening commands.\n",
    "\n",
    "We read the data from the file into the <code>trainingData</code> variable and finish by closing the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Class label: 5')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize data\n",
    "allValues = trainingData[0].split(',')   #Splits long records by commas into a Python list\n",
    "imageArray = numpy.asfarray(allValues[1:]).reshape(28, 28)\n",
    "\n",
    "plt.imshow(imageArray, cmap='Greys', interpolation='None')\n",
    "\n",
    "plt.title(\"Class label: \" + str(allValues[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract one record from the training data and split it into a list. This list is then transformed into a 28x28 <code><font color='orange'>numpy</font></code> array using <code><font color='green'>reshape</font></code>. \n",
    "\n",
    "We can plot the array as an image using <code><font color='orange'>pyplot</font></code>'s <code><font color='green'>imshow</font></code> function. As we can see, the result is a handwritten five!\n",
    "\n",
    "To see other digits, try modifying the index value in <code>trainingData[0]</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create instance of the neural network\n",
    "#Define arguments\n",
    "inputNodes = 784   \n",
    "hiddenNodes = 100  \n",
    "outputNodes = 10    \n",
    "learningRate = 0.3   \n",
    "\n",
    "classifier = NeuralNet(inputNodes, hiddenNodes, outputNodes, learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of what our data looks like, we can begin building the neural network. We want the network to scan an image and produce the correct class label. \n",
    "\n",
    "<ul>\n",
    "    <li><code>inputNodes</code> is set to 784, since we have 784 \"pixels\" in the input image</li>\n",
    "    <li><code>hiddenNodes</code> is set to 100, an arbitrary number. We chose 100 since it is between 10 and 784.</li>\n",
    "    <li><code>outputNodes</code> is set to 10. Since the possible class labels are the digits 0 through 9, there needs to be one output node per result.</li>\n",
    "    <li><code>learningRate</code> is set to an arbitrary value of 0.3. This can later be adjusted for more accuracy, if desired.</li>\n",
    "</ul>\n",
    "\n",
    "After setting all the variables we create the classifier from our neural network class.\n",
    "\n",
    "This can be hard to visualize at first. Essentially, we are sending the array of pixels through the input nodes, one for each pixel. Then, calculations are applied to the nodes and they are sent through the 100 hidden nodes. Finally, the results of those nodes are sent through the 10 output nodes, one for each digit. Of the 10 nodes, whichever has the highest value is the prediction.\n",
    "\n",
    "<img src=\"media/output.png\">\n",
    "<div align=\"center\"><b>Figure 3:</b> Output Node Interpretation </div>\n",
    "\n",
    "In the figure above, we can see that for the training instance <code>5</code>, the neural network's node for 5 has the highest value. Therefore, the network guesses the digit is five. Notice that with <code>9</code>, <code>4</code> also has an elevated value. Since 9s and 4s can look similar, the neural network predicted that it could be a four, but is most likely a nine. Here we can see that \"fuzzy logic\" in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale + split data and train neural network\n",
    "for record in trainingData:\n",
    "    #Split by commas\n",
    "    allValues = record.split(',')\n",
    "    \n",
    "    #Scale inputs\n",
    "    inputs = (numpy.asfarray(allValues[1:]) / 255.0 * 0.99) + 0.01\n",
    "    \n",
    "    #Create target output values\n",
    "    targets = numpy.zeros(outputNodes) + 0.01\n",
    "    targets[int(allValues[0])] = 0.99  #Class label\n",
    "    \n",
    "    classifier.train(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin training the neural network. We begin by iterating through all records in <code>trainingData</code>.\n",
    "\n",
    "First, we split the values in the record into a list. After this, we scale them between 0.01 and 1. In their original form, values in the records range from 0 to 255. Scaling them down makes the values much easier to work with.\n",
    "\n",
    "Following this, we extract our class labels and store them in <code>targets</code>. <code><font color='green'>zeros</font></code> fills the array with zeros, and we add 0.01 to avoid issues with multiplication by 0. Then, we convert the class label from a string to an integer.\n",
    "\n",
    "Finally, we train our classifier by using the <code><font color='green'>train</font></code> function, sending the inputs (images) and the class labels (target digits).\n",
    "\n",
    "When running the code, you may notice that nothing appears to happen. Training the classifier itself does not produce output, but we can verify the performance of the classifier to see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing and Verification</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import test data\n",
    "testDataFile = open(\"data/mnist_test.csv\", \"r\")\n",
    "testData = testDataFile.readlines()\n",
    "testDataFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the testing portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform test\n",
    "score = [] #scorecard for network performance\n",
    "\n",
    "for record in testData:\n",
    "    #Split record\n",
    "    allValues = record.split(',')\n",
    "    \n",
    "    #Separate class label\n",
    "    correctLabel = int(allValues[0])\n",
    "    \n",
    "    #Scale inputs\n",
    "    inputs = (numpy.asfarray(allValues[1:]) / 255.0 * 0.99) + 0.01\n",
    "    \n",
    "    #Query neural network\n",
    "    outputs = classifier.query(inputs)\n",
    "    \n",
    "    #Retrieve index of highest value corresponding to class label\n",
    "    label = numpy.argmax(outputs)\n",
    "\n",
    "    #Check accuracy\n",
    "    if(label == correctLabel):\n",
    "        score.append(1)\n",
    "    else:\n",
    "        score.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of how well the classifier performed, we can calculate accuracy scores. We can store this in an array called <code>score</code>. \n",
    "\n",
    "We iterate through each record of the testing data, splitting and scaling the data just like before. We also make sure to separate out the class label and store that in <code>correctLabel</code>.\n",
    "\n",
    "Using the <code>query</code> function of our neural network, we send the test data record as input to the classifier. This is stored in <code>output</code>. Knowing that the output node with the highest value is the prediction, we retrieve this value and store it in <code>label</code>. <code><font color='orange'>numpy</font></code>'s <code><font color='green'>argmax</font></code> easily retrieves that value from the output node list.\n",
    "\n",
    "To finish, we compare the predicted label with the correct label. If they match, we append 1 to <code>score</code>, and if not, we append 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 94.69999999999999%\n"
     ]
    }
   ],
   "source": [
    "#Calculate accuracy\n",
    "scoreArray = numpy.asarray(score)\n",
    "print(\"Accuracy = \" + str(100*(scoreArray.sum() / scoreArray.size)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <code>score</code> array, we can calculate a final accuracy percentage. We simply average the scores and display the result as a percentage.\n",
    "\n",
    "On this run, the classifier has an accuracy of 94.70%. That's great! As you can see, even a simple neural network can get great results. The class we created can be adapated for numerous applications, and can even be further improved for better accuracy.\n",
    "\n",
    "For example, we can tweak the learning rate of the network and compare accuracies with each run.\n",
    "\n",
    "<img src=\"media/learningRate.png\">\n",
    "<div align=\"center\"><b>Figure 3:</b> Accuracy with Different Learning Rates </div>\n",
    "\n",
    "We can see that a learning rate that's too high or too low decreases the accuracy of the system. It seems that 0.2 is a good rate. Try adjusting the learning rate above (where the classifier is created) and see how the accuracy changes.\n",
    "\n",
    "For more information about improving this neural network's efficiency and accuracy, see the third chapter of <i>Make Your Own Neural Network</i>. \n",
    "<br /><br /><br />\n",
    "<font size=\"2px\">\n",
    "All images and code adapted from Rashid, T. (2016). <i>Make your own neural network.</i> North Charleston, SC: CreateSpace Independent Publishing.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
